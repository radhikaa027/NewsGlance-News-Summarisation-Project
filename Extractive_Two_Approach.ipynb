{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-23T17:38:14.114766Z",
     "iopub.status.busy": "2024-11-23T17:38:14.114362Z",
     "iopub.status.idle": "2024-11-23T17:40:47.220624Z",
     "shell.execute_reply": "2024-11-23T17:40:47.219821Z",
     "shell.execute_reply.started": "2024-11-23T17:38:14.114719Z"
    },
    "id": "W_W3SWqqTeVv",
    "outputId": "8e2c6c41-60d7-413a-ae5e-639ee5c8e3d1",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Article 1:\n",
      "cnn  americans have been repeatedly shocked by school violence this year first in nevada where in october a student shot and killed a teacher and wounded two students before taking his own life then days later by the news that the body of a young teacher was found behind her school in massachusetts. the images of traumatized parents and a campus surrounded by police tape shake us profoundly  our hearts break for the families of those who died. for them this is the beginning of an unwanted journey. in my education research i have focused on the question of what happens in the lives of the people still connected to a school that has endured such a trauma long after the media and law enforcement move on. school shootings affect teachers school secretaries maintenance people and anyone else on campus at the time. how the school the community and families work together is critical to the longterm recovery of those who witnessed the nightmare. its important to consider two aspects of life after a school shooting. first what do teachers go through as the days stretch on and second what can we learn from past shootings to help the educators in sparks for teachers who witness school shootings one diagnosis stands out and it is wellknown in the field of psychology posttraumatic stress disorder or ptsd. teachers who see a shooting will experience the trauma of the activating event and a series of other emotions pain confusion guilt shame a questioning of selfworth fear anger depression and sometimes acute anxiety. they may question their very sense of how the world works around them. they struggle to find a sense of safety. teacher slayings hurt view of schools as safe havens . if ptsd is not dealt with the quality of the teachers life can degrade rapidly. psychiatric clinician bessel van der kolk has described how left untreated these changes can become permanent. victims of untreated ptsd can degrade into rigid thinking paranoia defensiveness overreactivity and health problems. this can have serious ramifications for students in their classrooms. they might find their teacher more and more withdrawn less willing to engage intellectually or emotionally defensive sarcastic inflexible and perhaps emotionally unstable. teacher absenteeism  meaning more substitute teachers  might become a problem. what can we learn from previous shootings that might help the educators in nevada the most important factor in recovery is quick access to professional crisis counseling. crisis counselors must discern how each individual witness deals with ptsd. in their trauma research psychologists james fauerbach and john lawrence found that adjustment to trauma is a dynamic process influenced by the intensity of the incident but also by pretrauma factors  for example family stability socioeconomic status. personal resilience which is effected by such things as social support personality and native coping abilities plays a role. this is why past experience teaches us its important to get quick professional psychological treatment tailored to each individual. more severe ptsd symptoms such as recurring nightmares can arise over time its crucial that counseling continue to be available and actively offered. after the first few weeks have passed witnesses still need support. one of the worst things anyone can do is pressure the witness to just get over it. past experience suggests that teachers who witness shootings and who have these layers of support from family and community recover more fully and quickly than those who are more isolated. and then there is time. each person heals at a different pace. there will be teachers who will be able to return to their duties in a week or so some may need months or years of support and therapy. in my own study i compared two teachers recovering after witnessing a school shooting. one was able to return within a week and had extended support from district and local mental health professionals for some time. a mental health professional checked in on him weekly even a year later. the other teacher had no such support. she was not even offered counseling. her world demonstrating van der kolks assertions descended into defensiveness and health problems. she was unable to function after a month back on the job. school administrators should understand that a teacher going back into the classroom may be an unknown their postshooting self may still be struggling in many ways. unfortunately a 2005 study found that 75 of school districts affected by campus shootings did not require counseling for teacher witnesses. coordinating a reentry plan with a professional psychologist can be fundamental to restoring a witness to a productive and healthy life. finally research indicates that the more connected a witness feels to the school to his or her family and to the community the better the possibility for longterm healing. school administrators by routinely fostering relationships through offcampus faculty parties softball games and other group activities could be setting up a safety net that could smooth over a transition for future trauma victims. one of the participants in my study regularly went on recreational events with coworkers. he mentioned how these same people felt connected to him and were there for him after the shooting. they knew him knew when he needed alone time and when he needed to be around people. while this cannot prevent a tragedy it can set up mechanisms to assist in healing. in the weeks and months that follow administrators community members family members and colleagues should be sensitive to events objects and other factors that may cause heightened anxiety and panic attacks. to guide teachers administrators will need professional advice from psychologists. beyond these things what can the rest of us do to help first of all be a part of your local school. teachers students administrators and other school workers need to feel valued and respected. be a part of the process to find ways to make schools safer. secondly encourage your local school leaders to develop school morale to develop crisis management plans and to foster a community feeling around the school. the opinions expressed in this commentary are solely those of edward mooney jr.\n",
      "\n",
      "Extractive Summary 1:\n",
      "cnn  americans have been repeatedly shocked by school violence this year first in nevada where in october a student shot and killed a teacher and wounded two students before taking his own life then days later by the news that the body of a young teacher was found behind her school in massachusetts. first what do teachers go through as the days stretch on and second what can we learn from past shootings to help the educators in sparks for teachers who witness school shootings one diagnosis stands out and it is wellknown in the field of psychology posttraumatic stress disorder or ptsd. teachers who see a shooting will experience the trauma of the activating event and a series of other emotions pain confusion guilt shame a questioning of selfworth fear anger depression and sometimes acute anxiety. in their trauma research psychologists james fauerbach and john lawrence found that adjustment to trauma is a dynamic process influenced by the intensity of the incident but also by pretrauma factors  for example family stability socioeconomic status. school administrators by routinely fostering relationships through offcampus faculty parties softball games and other group activities could be setting up a safety net that could smooth over a transition for future trauma victims.\n",
      "\n",
      "================================================================================\n",
      "Article 2:\n",
      "by . alex greig . published . 0939 est 2 january 2014 . . updated . 1300 est 2 january 2014 . a human skull has been found on a paper plate surrounded by beads in california. police responded to a tip of a sighting of human bones along grizzly peak boulevard in the grizzly peak area of berkley. officer johanna watson told cbs that police found the human skull out in the open on wednesday afternoon. scroll down for video . skull found the skull was found on grizzly peak boulevard a high ridge popular with hikers and mountain bikers . strange discovery police arrived to investigate the skull sighting around 540pm wednesday . little is known about the skull including its age but officer said it appeared to be more than several years old. according to the mercury news police do not believe the skull was of native american origin. . the alameda county coroner is examining the skull. grizzly peak is a summit in the berkley hills popular with hikers cyclists and tourists. part of me is not surprised for how many different types of people i see up here all the time local resident dakota defiore told cbs. mystery police say the skull appears to be old but have not released any further information stock image you know its dark and sometimes people drink up here. but part of me is kind of shocked you know never really thought driving up here id see a bunch of cop cars. police are investigating the bizarre discovery. they have not made any further information available but will provide an update on the case thursday morning.\n",
      "\n",
      "Extractive Summary 2:\n",
      "police responded to a tip of a sighting of human bones along grizzly peak boulevard in the grizzly peak area of berkley. officer johanna watson told cbs that police found the human skull out in the open on wednesday afternoon. part of me is not surprised for how many different types of people i see up here all the time local resident dakota defiore told cbs. mystery police say the skull appears to be old but have not released any further information stock image you know its dark and sometimes people drink up here. but part of me is kind of shocked you know never really thought driving up here id see a bunch of cop cars.\n",
      "\n",
      "================================================================================\n",
      "Article 3:\n",
      "fitting an airport into an area that measures just 2.3 square miles was never going to be easy task. thankfully gibraltar had a novel way of solving the issue. the airport which serves the british overseas territory has made full use of the minimal space and lack of flat land available by building its only runway through the heart of the peninsulas busiest road. scroll down for video . the history channel programme most extreme airports once ranked gibraltars airport as the fifth most extreme airport in the world . gibraltar airport has made full use of the minimal space and lack of flat land available by building its only runway through the heart of one of the peninsulas busiest roads . it means that cars travelling along winston churchill avenue must stop for planes several times a day. for about 10 minutes traffic stays at a standstill to allow a flights to depart and arrive from uk cities including london birmingham or manchester. a pair of barriers close ahead of every landing and departure which currently number around 30 a week all flying to and from the united kingdom. the airport is just 500 yards from the centre of gibraltar  the shortest commute of any major airport in the world. as well as serving cars and other vehicles the road also carries pedestrians in and out of gibraltar. there are currently three flights operated daily to gatwick and luton and three flights a week to manchester . it was in 2007 the government released plans for a new fourlane road that would divert traffic through a tunnel under the runway . as a result the history channel programme most extreme airports once ranked the airport the fifth most extreme airport in the world. gibraltar is a selfgoverning british overseas territory located on the southern end of the iberian peninsula and europe overlooking the strait of gibraltar. it has been under british sovereignty for over 300 years after it was captured by the uk in 1704. it was formally ceded by spain in the treaty of utrecht in 1713 although the spanish have long disputed the uks sovereignty arguing that the rock should be returned. as well as serving cars and other vehicles the road also carries pedestrians in and out of gibraltar . for about 10 minutes traffic stays at a standstill to allow a flights to depart and arrive from uk cities including london birmingham or manchester . pedestrians and cars are held at barriers on either side of the runway while a plane taxis across the fourlane winston churchill avenue after landing . the airport was constructed during world war ii when gibraltar was an important naval base for the british and was built over a horse racing track. originally opened in 1939 it was only an emergency airfield for the royal navys fleet air arm. however the runway was later extended into the bay of gibraltar using rock blasted from the historic rock of gibraltar which was being excavated by the military who were creating a series of tunnels inside. the fourlane main road was then added which leads from the centre of gibraltar through the airport and up to the spanish border which lies just north of the runway. at one stage the road across the runway was considered to be constraining operations at the airport because of the increase in traffic following the signing of the crdoba agreement in 2006. the airport is 500 yards from the centre of gibraltar  the shortest commute of any major airport in the world . while the average time the road is closed for an aircraft to land or depart is ten minutes on certain days the road is closed for over two hours . the agreement between the governments of spain the united kingdom and gibraltar opened up the airport to allow flights to and from spain which had previously been banned because of tensions between the two countries. prior to this agreement only three flights operated daily to gatwick and luton and three flights a week to manchester. the airport is frequently used by visitors and tourists travelling to or from neighbouring parts of the southern spain such as the costa del sol and handles more than 300000 visitors a year in addition to frequent cargo flights. on busy days some seven flights now arrive and depart. while the average time the road is closed for an aircraft to land or depart is ten minutes on certain days the road can be closed for over two hours. it was in 2007 the government released plans for a new fourlane road that would divert traffic through a tunnel under the runway. the road scheduled to open in 2009 has still not been completed.\n",
      "\n",
      "Extractive Summary 3:\n",
      "the airport which serves the british overseas territory has made full use of the minimal space and lack of flat land available by building its only runway through the heart of the peninsulas busiest road. it has been under british sovereignty for over 300 years after it was captured by the uk in 1704. it was formally ceded by spain in the treaty of utrecht in 1713 although the spanish have long disputed the uks sovereignty arguing that the rock should be returned. for about 10 minutes traffic stays at a standstill to allow a flights to depart and arrive from uk cities including london birmingham or manchester . at one stage the road across the runway was considered to be constraining operations at the airport because of the increase in traffic following the signing of the crdoba agreement in 2006. the airport is 500 yards from the centre of gibraltar  the shortest commute of any major airport in the world . the airport is frequently used by visitors and tourists travelling to or from neighbouring parts of the southern spain such as the costa del sol and handles more than 300000 visitors a year in addition to frequent cargo flights.\n",
      "\n",
      "================================================================================\n",
      "Article 4:\n",
      "by . paul sims . last updated at 123 am on 23rd december 2011 . as europes largest military base it is home to more than 12000 army personnel and their families. so it must have come as a surprise to the soldiers of catterick garrison when they discovered that the military wives charity single was nowhere to be seen in their local tesco. not least because the whole idea for the record came from a member of the garrisons own wags choir of servicemens wives and girlfriends. the military wives on oxford street on tuesday. their charity single wherever you are is currently outselling the rest of the top 20 combined . but surprise turned to anger when they were told by tesco that its store in the north yorkshire army base was too small to stock the song wherever you are  even though it was big enough for little mixs x factor song cannonball. the ban on the military wives single led to heartfelt protests from a battalion of disappointed locals. and last night tesco was beating a hasty retreat on its decision. retired headmistress glennis walton from nearby scotton tried to buy the single  which is outselling the rest of the top 20 combined and is expected to be the christmas number one  on wednesday. they told me the store isnt big enough she said. well its big enough to take the money from the military personnel. ive never been incandescent of scotton before but i feel very strongly about this. mrs walton was forced to buy the single online instead. another shopper jennie pinkney of nearby richmond was equally appalled when she and her husband tried to buy the single on monday. he was looking for the disc and he caught up with me and said youll not believe it  they dont have the single she said. the tesco in catterick claims it is too small to stock the charity single . i went up to the help desk and they told me the store was too small to stock the military wives single. i was furious about it. this is a tesco store that is in the heart of the biggest military base in western europe. they had the x factor girls single. i dont have anything against them good luck to them but military servicemens wives work at this tesco and military personnel use the store. given the price of the single its the kind of thing children would want to buy for their parents who are serving in the forces. catterick garrison is the largest british army garrison in the world with a population of around 12000 plus a large temporary population of soldiers. it is said to be the largest army base in europe. it is not a single fenced base but several separate barracks around which a town has developed. the garrison gained its first large supermarket a tesco in 2000 along with a retail park including a mcdonalds. we have a history of raf service in our family so i was appalled they did not stock the single. the girls on the help desk said they would pass my concerns higher up the chain after my complaint. local businessman alasdair macconachie who chairs the garrisons independent advisory panel said its a great shame they have not got behind this. they should be doing what they can to support it. in the face of the onslaught tesco was last night shipping copies of the single to the garrison. a spokesman said the supermarket chains customers tended to purchase and download singles online so it did not generally stock them in its stores. but he added this is a special charity project we are pleased to be supporting. the single is available in 205 of our stores and on december 26 it will be available in 458. the military wives were formed by choirmaster gareth malone for bbc2 programme the choir. he was given the idea by a member of catterick garrison wags choir. the single is raising money for the royal british legion and the soldiers sailors airmen and families association. the treasury said this week that it has agreed to give a sum equivalent to the vat on sales of the single to the appeal.\n",
      "\n",
      "Extractive Summary 4:\n",
      "but surprise turned to anger when they were told by tesco that its store in the north yorkshire army base was too small to stock the song wherever you are  even though it was big enough for little mixs x factor song cannonball. retired headmistress glennis walton from nearby scotton tried to buy the single  which is outselling the rest of the top 20 combined and is expected to be the christmas number one  on wednesday. local businessman alasdair macconachie who chairs the garrisons independent advisory panel said its a great shame they have not got behind this. a spokesman said the supermarket chains customers tended to purchase and download singles online so it did not generally stock them in its stores. the single is available in 205 of our stores and on december 26 it will be available in 458. the military wives were formed by choirmaster gareth malone for bbc2 programme the choir.\n",
      "\n",
      "================================================================================\n",
      "Article 5:\n",
      "a creative californian dad has raised the bar for halloween costumes by making his toddler a hilarious suit made from led lights along with a video that became an online sensation  with over 22 million views. now that stickfigure costume has been upgraded to feature ears like minnie mouse and glow in many colors. zoey was a huge hit when we went to disneyland dad royce hutain quipped on youtube where he posted the video of his adorable toddler zoey sporting her shiny new halloween costume. scroll down for video . family affair zoey and her family light up the night at disney world in their glowy zoey led suits created by california dad royce hutain whose led invention caused him newfound success . easy to spot no matter how far zoey is unmistakable in her adorable new minnie mouse led getup . playful wardrobe the comfortable led suit changes colors and bends according to the childs movements . hutain who is a photographer says hes also found himself in the led stick figure costume business following last years success and that while the new glow suit isnt for sale yet they will be sometime in the future on his website glowyzoey.com. for now viewers can enjoy precious footage of his daughter who he calls glowy zoey as light upminnie mouse illuminating cuteness in a dark room. not only can zoey light up any dim space she can also change the color of her leds by making a loud sound. twisted sifter reports that the 2014 version of the costume had the led strips hooked up to an arduino microcontroller. the microcontroller is responsive to sound and has a button in the back of one of the ears along with two potentiometers and a microphone in the front of the costume. the outfit has 372 leds and 12 different modes. the vibrant costume is a hit with kids and parents alike as its a surefire way to spot ones child in a crowd of costumed trickortreaters. grownups who have an inner child shining from within can also sport it on the outside with the adult stick figure version of glowy zoey. sudden success royce hutain made the stick figure suit for his daughter zoey last year just for fun but says he got into the led costume business following his creations mass success . fun that glows the led suit is enjoyable for children especially those who may be afraid of the dark . minnie mouse zoeys led costume is made to look like minnie mouse a popular disney character .\n",
      "\n",
      "Extractive Summary 5:\n",
      "a creative californian dad has raised the bar for halloween costumes by making his toddler a hilarious suit made from led lights along with a video that became an online sensation  with over 22 million views. zoey was a huge hit when we went to disneyland dad royce hutain quipped on youtube where he posted the video of his adorable toddler zoey sporting her shiny new halloween costume. family affair zoey and her family light up the night at disney world in their glowy zoey led suits created by california dad royce hutain whose led invention caused him newfound success . hutain who is a photographer says hes also found himself in the led stick figure costume business following last years success and that while the new glow suit isnt for sale yet they will be sometime in the future on his website glowyzoey.com. sudden success royce hutain made the stick figure suit for his daughter zoey last year just for fun but says he got into the led costume business following his creations mass success .\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the dataset\n",
    "train_df = pd.read_csv('/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/train.csv')\n",
    "validation_df = pd.read_csv('/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/validation.csv')\n",
    "test_df = pd.read_csv('/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/test.csv')\n",
    "\n",
    "# Sample the training data for computational efficiency\n",
    "train_df = train_df.sample(30000).reset_index(drop=True)\n",
    "\n",
    "# Define preprocessing functions\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing special characters, multiple spaces, and converting to lowercase.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = re.sub(r'[^a-zA-Z0-9. ]', '', text)  # Remove special characters\n",
    "    return text\n",
    "\n",
    "def preprocess_and_tokenize(text):\n",
    "    \"\"\"Preprocess the text and tokenize into sentences.\"\"\"\n",
    "    cleaned_text = clean_text(text)\n",
    "    sentences = sent_tokenize(cleaned_text)\n",
    "    return sentences\n",
    "\n",
    "# Preprocess articles\n",
    "train_df['sentences'] = train_df['article'].apply(preprocess_and_tokenize)\n",
    "\n",
    "# Define a function for extractive summarization\n",
    "def extractive_summary(article_sentences, top_n=5):\n",
    "    \"\"\"\n",
    "    Perform extractive summarization using TF-IDF to score sentences.\n",
    "    - `article_sentences`: List of sentences in the article\n",
    "    - `top_n`: Number of sentences to extract for the summary\n",
    "    \"\"\"\n",
    "    # Flatten the list of sentences into a single text for vectorization\n",
    "    flat_text = ' '.join(article_sentences)\n",
    "\n",
    "    # Initialize TF-IDF Vectorizer\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(article_sentences)\n",
    "\n",
    "    # Compute sentence scores as the sum of TF-IDF weights for each word\n",
    "    sentence_scores = tfidf_matrix.sum(axis=1).A.flatten()\n",
    "\n",
    "    # Rank sentences by their scores and extract the top N\n",
    "    ranked_sentences = sorted(\n",
    "        ((score, idx) for idx, score in enumerate(sentence_scores)),\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    # Select the top N sentences\n",
    "    top_sentence_indices = [idx for _, idx in ranked_sentences[:top_n]]\n",
    "\n",
    "    # Return the top sentences in the order they appear in the text\n",
    "    summary = ' '.join([article_sentences[idx] for idx in sorted(top_sentence_indices)])\n",
    "    return summary\n",
    "\n",
    "# Apply summarization on a subset of the training data\n",
    "sample_articles = train_df['sentences'].head(5)  # Select a few articles for summarization\n",
    "\n",
    "# Generate summaries\n",
    "summaries = []\n",
    "for article_sentences in sample_articles:\n",
    "    summaries.append(extractive_summary(article_sentences, top_n=5))\n",
    "\n",
    "# Display original and summarized text for comparison\n",
    "for idx, (original, summary) in enumerate(zip(sample_articles, summaries)):\n",
    "    print(f\"Article {idx+1}:\\n{' '.join(original)}\\n\")\n",
    "    print(f\"Extractive Summary {idx+1}:\\n{summary}\\n\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "# Save the summaries to a CSV file for evaluation\n",
    "train_df['summary'] = train_df['sentences'].apply(lambda x: extractive_summary(x, top_n=5))\n",
    "train_df[['article', 'summary']].to_csv('train_summaries.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T17:42:58.188862Z",
     "iopub.status.busy": "2024-11-23T17:42:58.188049Z",
     "iopub.status.idle": "2024-11-23T17:43:08.821482Z",
     "shell.execute_reply": "2024-11-23T17:43:08.820373Z",
     "shell.execute_reply.started": "2024-11-23T17:42:58.188828Z"
    },
    "id": "BFm5MotcTeV1",
    "outputId": "8340e89a-1d19-4f01-c27a-e01f676b1a64",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge-score) (3.2.4)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.16.0)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=02c0973366b62c63af4a1c991435fc44d0df4edab75e3cc0b0ffb5892bb7836d\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rouge-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T17:43:08.823747Z",
     "iopub.status.busy": "2024-11-23T17:43:08.823441Z",
     "iopub.status.idle": "2024-11-23T17:43:08.877995Z",
     "shell.execute_reply": "2024-11-23T17:43:08.877187Z",
     "shell.execute_reply.started": "2024-11-23T17:43:08.823719Z"
    },
    "id": "rRO7zhvFTeV-",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T17:43:20.606658Z",
     "iopub.status.busy": "2024-11-23T17:43:20.606045Z",
     "iopub.status.idle": "2024-11-23T17:43:41.443316Z",
     "shell.execute_reply": "2024-11-23T17:43:41.442183Z",
     "shell.execute_reply.started": "2024-11-23T17:43:20.606610Z"
    },
    "id": "FRam-vY5TeV_",
    "outputId": "5df5324a-ac5f-45ca-bb77-7ad943832758",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.66.4)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nltk\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.2.4\n",
      "    Uninstalling nltk-3.2.4:\n",
      "      Successfully uninstalled nltk-3.2.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "preprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nltk-3.9.1\n",
      "Collecting nltk==3.5\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk==3.5) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk==3.5) (1.4.2)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from nltk==3.5) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk==3.5) (4.66.4)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434678 sha256=665d300e5bfdf22f1327d8b557e0d3c0362685511f8be560d00d2d9bc188095f\n",
      "  Stored in directory: /root/.cache/pip/wheels/35/ab/82/f9667f6f884d272670a15382599a9c753a1dfdc83f7412e37d\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.9.1\n",
      "    Uninstalling nltk-3.9.1:\n",
      "      Successfully uninstalled nltk-3.9.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "preprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.5 which is incompatible.\n",
      "textblob 0.18.0.post0 requires nltk>=3.8, but you have nltk 3.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nltk-3.5\n"
     ]
    }
   ],
   "source": [
    "# Uninstall the previous installed nltk library\n",
    "\n",
    "!pip install -U nltk\n",
    "\n",
    "\n",
    "\n",
    "# This upgraded nltkto version 3.5 in which meteor_score is there.\n",
    "\n",
    "!pip install nltk==3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T17:43:41.446551Z",
     "iopub.status.busy": "2024-11-23T17:43:41.445709Z",
     "iopub.status.idle": "2024-11-23T17:43:49.972923Z",
     "shell.execute_reply": "2024-11-23T17:43:49.972050Z",
     "shell.execute_reply.started": "2024-11-23T17:43:41.446507Z"
    },
    "id": "2j8MhAfKTeWA",
    "outputId": "fe06a636-9b02-4393-b292-6c28ce44b85d",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.25.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T17:43:49.974627Z",
     "iopub.status.busy": "2024-11-23T17:43:49.974257Z",
     "iopub.status.idle": "2024-11-23T17:43:58.571447Z",
     "shell.execute_reply": "2024-11-23T17:43:58.570578Z",
     "shell.execute_reply.started": "2024-11-23T17:43:49.974594Z"
    },
    "id": "MDPyVk1jTeWA",
    "outputId": "23a7e6e5-9ef2-496d-975a-6dd0019c3246",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert_score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.4.0)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.2.2)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from bert_score) (4.45.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bert_score) (1.26.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /opt/conda/lib/python3.10/site-packages (from bert_score) (4.66.4)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from bert_score) (3.7.5)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from bert_score) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->bert_score) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2024.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.15.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.25.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (2024.5.15)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.20.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (10.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (2024.8.30)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n",
      "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bert_score\n",
      "Successfully installed bert_score-0.3.13\n"
     ]
    }
   ],
   "source": [
    "!pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T17:43:58.573823Z",
     "iopub.status.busy": "2024-11-23T17:43:58.573546Z",
     "iopub.status.idle": "2024-11-23T17:44:02.833715Z",
     "shell.execute_reply": "2024-11-23T17:44:02.832770Z",
     "shell.execute_reply.started": "2024-11-23T17:43:58.573798Z"
    },
    "id": "WfvANsC1TeWB",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "from bert_score import score as bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T17:45:29.365982Z",
     "iopub.status.busy": "2024-11-23T17:45:29.365609Z",
     "iopub.status.idle": "2024-11-23T17:45:41.442774Z",
     "shell.execute_reply": "2024-11-23T17:45:41.441795Z",
     "shell.execute_reply.started": "2024-11-23T17:45:29.365949Z"
    },
    "id": "x9Q8t2J0TeWB",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "test_df[\"clean_article\"] = test_df[\"article\"].apply(lambda x: [sent_tokenize(x.lower())])\n",
    "test_df[\"clean_highlights\"] = test_df[\"highlights\"].apply(lambda x: [sent_tokenize(x.lower())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T17:49:51.645728Z",
     "iopub.status.busy": "2024-11-23T17:49:51.645367Z",
     "iopub.status.idle": "2024-11-23T17:49:51.653221Z",
     "shell.execute_reply": "2024-11-23T17:49:51.652375Z",
     "shell.execute_reply.started": "2024-11-23T17:49:51.645692Z"
    },
    "id": "26W3M_b3TeWB",
    "outputId": "94645b25-cd66-48e0-b8a3-2dee4b38af83",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')  # Ensure this is also downloaded for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T17:51:50.148686Z",
     "iopub.status.busy": "2024-11-23T17:51:50.148041Z",
     "iopub.status.idle": "2024-11-23T17:51:51.097331Z",
     "shell.execute_reply": "2024-11-23T17:51:51.096511Z",
     "shell.execute_reply.started": "2024-11-23T17:51:50.148654Z"
    },
    "id": "9jEX7LQLTeWC",
    "outputId": "d8c07b4c-1476-4899-9826-568bf58a7d83",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /kaggle/working/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /kaggle/working/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /kaggle/working/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "NLTK data directory set to: /kaggle/working/nltk_data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import nltk\n",
    "\n",
    "\n",
    "\n",
    "# Set the directory for NLTK data\n",
    "\n",
    "nltk_data_dir = '/kaggle/working/nltk_data'\n",
    "\n",
    "os.makedirs(nltk_data_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "# Set the NLTK_DATA environment variable to point to the new directory\n",
    "\n",
    "os.environ['NLTK_DATA'] = nltk_data_dir\n",
    "\n",
    "\n",
    "\n",
    "# Download the required NLTK packages\n",
    "\n",
    "nltk.download('wordnet', download_dir=nltk_data_dir)\n",
    "\n",
    "nltk.download('omw-1.4', download_dir=nltk_data_dir)  # Optional for better word matching\n",
    "\n",
    "nltk.download('punkt', download_dir=nltk_data_dir)\n",
    "\n",
    "\n",
    "\n",
    "# Check if the downloads are successful by verifying the corpus location\n",
    "\n",
    "print(\"NLTK data directory set to:\", nltk_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ClyYS85BTeWI",
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T17:52:10.385110Z",
     "iopub.status.busy": "2024-11-23T17:52:10.384770Z",
     "iopub.status.idle": "2024-11-23T17:52:10.769918Z",
     "shell.execute_reply": "2024-11-23T17:52:10.768887Z",
     "shell.execute_reply.started": "2024-11-23T17:52:10.385081Z"
    },
    "id": "VdTr55qOTeWI",
    "outputId": "c238929f-d36e-44e3-e99a-e808871048e0",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted WordNet corpus.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "\n",
    "\n",
    "# Path to the WordNet zip file\n",
    "\n",
    "wordnet_zip_path = '/kaggle/working/nltk_data/corpora/wordnet.zip'\n",
    "\n",
    "wordnet_extract_path = '/kaggle/working/nltk_data/corpora/'\n",
    "\n",
    "\n",
    "\n",
    "# Extract if it exists\n",
    "\n",
    "if os.path.exists(wordnet_zip_path):\n",
    "\n",
    "    with zipfile.ZipFile(wordnet_zip_path, 'r') as zip_ref:\n",
    "\n",
    "        zip_ref.extractall(wordnet_extract_path)\n",
    "\n",
    "    print(\"Extracted WordNet corpus.\")\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"WordNet zip file not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T17:52:27.463668Z",
     "iopub.status.busy": "2024-11-23T17:52:27.463295Z",
     "iopub.status.idle": "2024-11-23T17:52:27.469051Z",
     "shell.execute_reply": "2024-11-23T17:52:27.468119Z",
     "shell.execute_reply.started": "2024-11-23T17:52:27.463641Z"
    },
    "id": "65CTCEpHTeWJ",
    "outputId": "2e4364b7-10ca-4d2e-9dc3-8b17f644bc62",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordNet corpus found in: /kaggle/working/nltk_data/corpora/wordnet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# Path to check if the WordNet corpus exists\n",
    "\n",
    "wordnet_path = '/kaggle/working/nltk_data/corpora/wordnet'\n",
    "\n",
    "if os.path.exists(wordnet_path):\n",
    "\n",
    "    print(\"WordNet corpus found in:\", wordnet_path)\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"WordNet corpus not found. Please check the directory structure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T17:52:39.426161Z",
     "iopub.status.busy": "2024-11-23T17:52:39.425818Z",
     "iopub.status.idle": "2024-11-23T17:52:40.888784Z",
     "shell.execute_reply": "2024-11-23T17:52:40.887915Z",
     "shell.execute_reply.started": "2024-11-23T17:52:39.426130Z"
    },
    "id": "vvUxhZ7aTeWK",
    "outputId": "84419961-b8b8-4466-b584-7acd03cc3708",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /kaggle/working/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /kaggle/working/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /kaggle/working/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "WordNet loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Verify the WordNet corpus\n",
    "\n",
    "try:\n",
    "\n",
    "    # Test loading wordnet explicitly\n",
    "\n",
    "    wordnet.ensure_loaded()\n",
    "\n",
    "    print(\"WordNet loaded successfully.\")\n",
    "\n",
    "except LookupError as e:\n",
    "\n",
    "    print(\"Failed to load WordNet:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "b4b572b947e64416b810b81cb9acfa7a",
      "46ff51866e8f416782440363a4839d25",
      "0cd1437da6484ba08a186f8bfec8c616",
      "a0146c332cc64678be8af2a4f0b8c506",
      "aa2cdf3d9d5c4c108c3a85c9788235ad",
      "d3efbaf9a78e4dc0ac56052f6661aec7"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-11-23T17:52:44.383663Z",
     "iopub.status.busy": "2024-11-23T17:52:44.383245Z",
     "iopub.status.idle": "2024-11-23T18:02:21.242460Z",
     "shell.execute_reply": "2024-11-23T18:02:21.241503Z",
     "shell.execute_reply.started": "2024-11-23T17:52:44.383627Z"
    },
    "id": "OK2n-oAGTeWK",
    "outputId": "23ddb437-f547-49c3-bbe6-20ef000e3189",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b572b947e64416b810b81cb9acfa7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46ff51866e8f416782440363a4839d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd1437da6484ba08a186f8bfec8c616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0146c332cc64678be8af2a4f0b8c506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa2cdf3d9d5c4c108c3a85c9788235ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3efbaf9a78e4dc0ac56052f6661aec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'ROUGE-1': 35.82, 'ROUGE-2': 16.7, 'ROUGE-L': 23.16, 'METEOR': 37.53, 'BERTScore (F1)': 86.69}\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from evaluate import load\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load metrics\n",
    "rouge_metric = load(\"rouge\")\n",
    "meteor_metric = load(\"meteor\")\n",
    "bertscore_metric = load(\"bertscore\")\n",
    "\n",
    "\n",
    "# Initialize the summarizer\n",
    "text_rank_summarizer = extractive_summary()\n",
    "\n",
    "# Define evaluation function\n",
    "def evaluate_performance(dataset):\n",
    "    # Convert tokenized sentences back to strings\n",
    "    references = [' '.join([' '.join(sentence) for sentence in highlight]) for highlight in dataset[\"clean_highlights\"]]\n",
    "    predictions = [' '.join([' '.join(sentence) for sentence in article]) for article in dataset[\"clean_article\"]]\n",
    "\n",
    "    # Generate summaries for each article\n",
    "    generated_summaries = []\n",
    "    for _, row in dataset.iterrows():\n",
    "        article_text = ' '.join([' '.join(sentence) for sentence in row[\"clean_article\"]])\n",
    "        summary = text_rank_summarizer.generate_summary(article_text)  # Generate a summary for each article\n",
    "        generated_summaries.append(summary)\n",
    "\n",
    "    # Compute ROUGE Scores\n",
    "    rouge_scores = rouge_metric.compute(predictions=generated_summaries, references=references)\n",
    "    rouge_results = {\n",
    "        \"ROUGE-1\": round(rouge_scores[\"rouge1\"] * 100, 2),\n",
    "        \"ROUGE-2\": round(rouge_scores[\"rouge2\"] * 100, 2),\n",
    "        \"ROUGE-L\": round(rouge_scores[\"rougeL\"] * 100, 2),\n",
    "        \"ROUGE-Lsum\": round(rouge_scores[\"rougeLsum\"] * 100, 2)\n",
    "    }\n",
    "\n",
    "    # Compute METEOR Score\n",
    "    meteor_score = meteor_metric.compute(predictions=generated_summaries, references=references)\n",
    "    meteor_result = round(meteor_score[\"meteor\"] * 100, 2)\n",
    "\n",
    "    # Compute BERTScore\n",
    "    bertscore_result = bertscore_metric.compute(predictions=generated_summaries, references=references, lang=\"en\")\n",
    "    bertscore_f1 = round(np.mean(bertscore_result[\"f1\"]) * 100, 2)\n",
    "\n",
    "    # Display the results\n",
    "    evaluation_results = {\n",
    "        \"ROUGE-1\": rouge_results[\"ROUGE-1\"],\n",
    "        \"ROUGE-2\": rouge_results[\"ROUGE-2\"],\n",
    "        \"ROUGE-L\": rouge_results[\"ROUGE-L\"],\n",
    "        \"METEOR\": meteor_result,\n",
    "        \"BERTScore (F1)\": bertscore_f1\n",
    "    }\n",
    "\n",
    "    return evaluation_results\n",
    "\n",
    "# Example usage: Evaluate on the test dataset\n",
    "# Ensure 'clean_article' and 'clean_highlights' columns exist and are preprocessed as required.\n",
    "results = evaluate_performance(test_df)\n",
    "print(\"Evaluation Results:\", results)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1654566,
     "sourceId": 2734496,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
