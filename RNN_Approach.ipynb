{
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 8240,
          "sourceType": "datasetVersion",
          "datasetId": 5504
        },
        {
          "sourceId": 2734496,
          "sourceType": "datasetVersion",
          "datasetId": 1654566
        },
        {
          "sourceId": 33960457,
          "sourceType": "kernelVersion"
        }
      ],
      "dockerImageVersionId": 30787,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 4318.498615,
      "end_time": "2024-11-08T19:08:59.987949",
      "environment_variables": {},
      "exception": true,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-11-08T17:57:01.489334",
      "version": "2.6.0"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Abstractive Text Summarization Using RNN"
      ],
      "metadata": {
        "id": "f2xyt2QDQ0Tv",
        "papermill": {
          "duration": 0.010323,
          "end_time": "2024-11-08T17:57:04.401940",
          "exception": false,
          "start_time": "2024-11-08T17:57:04.391617",
          "status": "completed"
        },
        "tags": []
      },
      "id": "f2xyt2QDQ0Tv"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"hello\")"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.022361,
          "end_time": "2024-11-08T17:57:04.435191",
          "exception": false,
          "start_time": "2024-11-08T17:57:04.412830",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-11-12T12:51:19.165224Z",
          "iopub.execute_input": "2024-11-12T12:51:19.165915Z",
          "iopub.status.idle": "2024-11-12T12:51:19.170905Z",
          "shell.execute_reply.started": "2024-11-12T12:51:19.165878Z",
          "shell.execute_reply": "2024-11-12T12:51:19.169842Z"
        },
        "trusted": true,
        "id": "y_1DAcDeDVsz",
        "outputId": "209eb4ba-50ad-44ce-c87a-2b3dac1d1725"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "hello\n",
          "output_type": "stream"
        }
      ],
      "id": "y_1DAcDeDVsz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Module Imports"
      ],
      "metadata": {
        "id": "X8RELhnh3uYa",
        "papermill": {
          "duration": 0.009233,
          "end_time": "2024-11-08T17:57:04.453966",
          "exception": false,
          "start_time": "2024-11-08T17:57:04.444733",
          "status": "completed"
        },
        "tags": []
      },
      "id": "X8RELhnh3uYa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1"
      ],
      "metadata": {
        "id": "swNEWveIDVs5"
      },
      "id": "swNEWveIDVs5"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from matplotlib import pyplot as plt\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import glob\n",
        "import itertools\n",
        "import pickle\n",
        "import re\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "!pip install rouge-score"
      ],
      "metadata": {
        "id": "N3f5SfJj3xD7",
        "outputId": "9d811710-40b5-4858-e9e6-4322bc056374",
        "papermill": {
          "duration": 23.356159,
          "end_time": "2024-11-08T17:57:27.819820",
          "exception": false,
          "start_time": "2024-11-08T17:57:04.463661",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-11-12T12:51:20.334000Z",
          "iopub.execute_input": "2024-11-12T12:51:20.334856Z",
          "iopub.status.idle": "2024-11-12T12:51:40.601259Z",
          "shell.execute_reply.started": "2024-11-12T12:51:20.334816Z",
          "shell.execute_reply": "2024-11-12T12:51:40.600176Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\nCollecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge-score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.16.0)\nBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=8a8f0925ad0cf49b17561c2df8127b8d0a8b27ceaeeaa9664a50aa685e78b832\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge-score\nInstalling collected packages: rouge-score\nSuccessfully installed rouge-score-0.1.2\n",
          "output_type": "stream"
        }
      ],
      "id": "N3f5SfJj3xD7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read Data and Data Preprocessing"
      ],
      "metadata": {
        "id": "u9Kdv0HHRBca",
        "papermill": {
          "duration": 0.01019,
          "end_time": "2024-11-08T17:57:27.840824",
          "exception": false,
          "start_time": "2024-11-08T17:57:27.830634",
          "status": "completed"
        },
        "tags": []
      },
      "id": "u9Kdv0HHRBca"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2"
      ],
      "metadata": {
        "id": "H4q0UMCNDVs9"
      },
      "id": "H4q0UMCNDVs9"
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=pd.read_csv('/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/train.csv')\n",
        "test_data=pd.read_csv('/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/test.csv')\n",
        "val_data=pd.read_csv('/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/validation.csv')"
      ],
      "metadata": {
        "papermill": {
          "duration": 30.833752,
          "end_time": "2024-11-08T17:57:58.685001",
          "exception": false,
          "start_time": "2024-11-08T17:57:27.851249",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-11-12T12:51:40.603533Z",
          "iopub.execute_input": "2024-11-12T12:51:40.604274Z",
          "iopub.status.idle": "2024-11-12T12:52:08.681613Z",
          "shell.execute_reply.started": "2024-11-12T12:51:40.604224Z",
          "shell.execute_reply": "2024-11-12T12:52:08.680689Z"
        },
        "trusted": true,
        "id": "wTFMQoFsDVs9"
      },
      "execution_count": null,
      "outputs": [],
      "id": "wTFMQoFsDVs9"
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting train data\n",
        "X_Train = train_data['article'].values[:100000]\n",
        "Y_Train = train_data['highlights'].values[:100000]\n",
        "\n",
        "# Splitting test data\n",
        "X_Test = test_data['article'].values\n",
        "Y_Test = test_data['highlights'].values\n",
        "\n",
        "# Splitting validation data\n",
        "X_Val = val_data['article'].values\n",
        "Y_Val = val_data['highlights'].values"
      ],
      "metadata": {
        "id": "BEqMLByJ5ixs",
        "papermill": {
          "duration": 0.023287,
          "end_time": "2024-11-08T17:57:58.719901",
          "exception": false,
          "start_time": "2024-11-08T17:57:58.696614",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-11-12T12:52:08.682808Z",
          "iopub.execute_input": "2024-11-12T12:52:08.683139Z",
          "iopub.status.idle": "2024-11-12T12:52:08.693061Z",
          "shell.execute_reply.started": "2024-11-12T12:52:08.683104Z",
          "shell.execute_reply": "2024-11-12T12:52:08.692025Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "BEqMLByJ5ixs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "STep 3"
      ],
      "metadata": {
        "id": "chlNS6_FDVs-"
      },
      "id": "chlNS6_FDVs-"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "import itertools\n",
        "import re\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.017821,
          "end_time": "2024-11-08T17:57:58.748144",
          "exception": false,
          "start_time": "2024-11-08T17:57:58.730323",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-11-12T12:52:08.695322Z",
          "iopub.execute_input": "2024-11-12T12:52:08.695610Z",
          "iopub.status.idle": "2024-11-12T12:52:08.702706Z",
          "shell.execute_reply.started": "2024-11-12T12:52:08.695580Z",
          "shell.execute_reply": "2024-11-12T12:52:08.701925Z"
        },
        "trusted": true,
        "id": "gIF3M0LtDVs_"
      },
      "execution_count": null,
      "outputs": [],
      "id": "gIF3M0LtDVs_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4"
      ],
      "metadata": {
        "id": "tPZL-QXJDVs_"
      },
      "id": "tPZL-QXJDVs_"
    },
    {
      "cell_type": "code",
      "source": [
        "drive_prefix = '/kaggle/working/'  # Kaggle's working directory for saving outputs\n",
        "\n",
        "# STOP_WORDS = set(stopwords.words('english'))\n",
        "STOP_WORDS = set()\n",
        "EMB_SIZE = 300\n",
        "GLOVE_EMB = '/kaggle/input/glove6b300dtxt/glove.6B.300d.txt'\n",
        "BATCH_SIZE = 32\n",
        "EPSILON = 0.5 # for deciding between feeding (model's output OR target) as input\n",
        "\n",
        "START_CHAR = 'starttoken'\n",
        "END_CHAR = 'endtoken'\n",
        "PAD_CHAR = 'padtoken'\n",
        "FORCE_CREATE_DICT = True\n",
        "MAX_ARTICLE_LEN = 300 # the article can have at most 300 tokens\n",
        "MAX_DECODER_OUTPUT = 100 # labels can have at most 100 tokens\n",
        "\n",
        "# NN Hyper-parameters\n",
        "E_HIDDEN_DIM = 200\n",
        "D_HIDDEN_DIM = 200\n",
        "\n",
        "EP = 4400\n",
        "PRINT_EVERY_EP = 50\n",
        "SAVE_MODEL_EVERY_EP = 500\n",
        "FORCE_CREATE_DICT = True # force to recreate the word features from scratch\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Create model directory if it doesn’t exist\n",
        "def save_checkpoint(ep, e_model, d_model, e_optim, d_optim, loss, model_name):\n",
        "    model_dir = f'{drive_prefix}models/{model_name}/'\n",
        "    os.makedirs(model_dir, exist_ok=True)  # Ensure directory exists\n",
        "\n",
        "    torch.save({\n",
        "        'epoch': ep,\n",
        "        'encoder_model': e_model.state_dict(),\n",
        "        'decoder_model': d_model.state_dict(),\n",
        "        'encoder_optimizer': e_optim.state_dict(),\n",
        "        'decoder_optimizer': d_optim.state_dict(),\n",
        "        'loss': loss\n",
        "    }, os.path.join(model_dir, f'checkpoint_{ep}'))\n",
        "    print(f'Checkpoint saved at epoch {ep}')\n"
      ],
      "metadata": {
        "id": "z-_Di8K832w4",
        "outputId": "31d96866-21d7-4101-ee7a-7b11b1c3a83c",
        "papermill": {
          "duration": 0.102647,
          "end_time": "2024-11-08T17:57:58.861105",
          "exception": false,
          "start_time": "2024-11-08T17:57:58.758458",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-11-12T12:52:08.704043Z",
          "iopub.execute_input": "2024-11-12T12:52:08.704859Z",
          "iopub.status.idle": "2024-11-12T12:52:08.771715Z",
          "shell.execute_reply.started": "2024-11-12T12:52:08.704805Z",
          "shell.execute_reply": "2024-11-12T12:52:08.770731Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "cuda\n",
          "output_type": "stream"
        }
      ],
      "id": "z-_Di8K832w4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "step 5\n"
      ],
      "metadata": {
        "id": "pjoJU93jDVtB"
      },
      "id": "pjoJU93jDVtB"
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(s):\n",
        "    s = s.lower().strip()\n",
        "    s = s.replace('<t>', '').replace('</t>', '').replace('\\n', '')\n",
        "    s = s.replace('-lrb-', '').replace('-rrb-', '')\n",
        "    s = re.sub(r'\\([^)]*\\)', '', s)\n",
        "    s = re.sub('\"','', s)\n",
        "    s = re.sub(r\"'s\\b\",\"\",s)\n",
        "    s = re.sub(\"[^a-zA-Z]\", \" \", s)\n",
        "    # s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    # s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "\n",
        "def create_word_vec(input_text, input_label, force_create):\n",
        "    word_idx_dict = {} # { word : the index of that word in the dictionary}\n",
        "    idx_word_dict = {} # { index of word : word }\n",
        "\n",
        "    # 1. Create mapping between words and the corresponding embedding values\n",
        "    embed_file_path = drive_prefix + f'{EMB_SIZE}d_embed_dict'\n",
        "    if os.path.exists(embed_file_path) and not force_create:\n",
        "        print('Embedding dictionary exists, loading from file...')\n",
        "        embedding_dict = pickle.load(open(embed_file_path, 'rb'))\n",
        "    else:\n",
        "        embedding_dict = {}\n",
        "\n",
        "        for line in glove:\n",
        "            tokens = line.split()\n",
        "            embedding_dict[tokens[0]] = np.array(tokens[1:], dtype='float32')\n",
        "        pickle.dump(embedding_dict, open(embed_file_path, 'wb'))  # Saving to `/kaggle/working`\n",
        "        print('Saved embedding dictionary')\n",
        "    # 2. Tokenize the input_text and labels\n",
        "    if os.path.exists(drive_prefix + 'train_word_idx_dict') and not force_create:\n",
        "        print('Word-to-index dictionary exists, loading from file...')\n",
        "        word_idx_dict = pickle.load(open(drive_prefix + 'train_word_idx_dict', 'rb'))\n",
        "    if os.path.exists(drive_prefix + 'train_idx_word_dict') and not force_create:\n",
        "        print('Index-to-word dictionary exists, loading from file...')\n",
        "        idx_word_dict = pickle.load(open(drive_prefix + 'train_idx_word_dict', 'rb'))\n",
        "    else:\n",
        "        unique_tokens = set([])\n",
        "        for line in input_text:\n",
        "            unique_tokens = unique_tokens.union(word_tokenize(line))\n",
        "        for line in input_label:\n",
        "            unique_tokens = unique_tokens.union(word_tokenize(line))\n",
        "\n",
        "        for token in unique_tokens:\n",
        "            word_idx_dict[token] = len(word_idx_dict)\n",
        "\n",
        "        # 2.1 Add in the special tokens to the dictionary, note that the START_CHAR and END_CHAR have been added\n",
        "        # during the preprocessing stage\n",
        "        word_idx_dict[PAD_CHAR] = len(word_idx_dict)\n",
        "\n",
        "        idx_word_dict = dict(zip(word_idx_dict.values(), word_idx_dict.keys()))\n",
        "\n",
        "    # 3. Build the word vector for all the words in our dictionary\n",
        "    if os.path.exists(drive_prefix + 'train_word_vector') and not force_create:\n",
        "        print('Word Vector exists, loading from file...')\n",
        "        word_vector = pickle.load(open(drive_prefix + 'train_word_vector', 'rb'))\n",
        "    else:\n",
        "        word_vector = []\n",
        "        for idx, token in idx_word_dict.items():\n",
        "            if token in embedding_dict:\n",
        "                word_vector.append(embedding_dict[token])\n",
        "            # Append the special tokens to the word vector and assign random values\n",
        "            elif token in [START_CHAR, END_CHAR, PAD_CHAR]:\n",
        "                word_vector.append(np.random.normal(0, 1, EMB_SIZE))\n",
        "            # if the token doesn't have an embedding, we set to 0\n",
        "            else:\n",
        "                word_vector.append(np.zeros([EMB_SIZE]))\n",
        "\n",
        "    ## Save the dictionaries\n",
        "    pickle.dump(word_idx_dict, open(drive_prefix + 'train_word_idx_dict', 'wb'))\n",
        "    pickle.dump(idx_word_dict, open(drive_prefix + 'train_idx_word_dict', 'wb'))\n",
        "    pickle.dump(word_vector, open(drive_prefix + 'train_word_vector', 'wb'))\n",
        "\n",
        "    # The index in word_vec corresponds to the article index in the original X_Test array\n",
        "    return np.array(word_vector), word_idx_dict, idx_word_dict\n",
        "\n",
        "def sentence_to_idx(sentence, word_to_idx):\n",
        "    tokens = word_tokenize(sentence)\n",
        "    return [word_to_idx[token] for token in tokens if token in word_to_idx]\n",
        "\n",
        "def decontracted(text):\n",
        "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"that is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"n'\", \"ng\", text)\n",
        "    text = re.sub(r\"'bout\", \"about\", text)\n",
        "    text = re.sub(r\"'til\", \"until\", text)\n",
        "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
        "    return text\n",
        "\n",
        "def pre_process(column, max_len):\n",
        "    preprocessed_text = []\n",
        "    # tqdm is for printing the status bar\n",
        "    for sentence in tqdm(column.astype(str)):\n",
        "        sent = decontracted(sentence)\n",
        "        sent = sent.replace('\\\\r', ' ')\n",
        "        sent = sent.replace('\\\\\"', ' ')\n",
        "        sent = sent.replace('\\\\n', ' ')\n",
        "        sent = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", sent)\n",
        "        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
        "        sent=re.sub('<[^>]*>', '', sent)\n",
        "        # trim longer articles\n",
        "        sent = sent.lower().strip()\n",
        "        sent = ' '.join(sent.split()[:max_len])\n",
        "        preprocessed_text.append(sent)\n",
        "    return preprocessed_text\n",
        "\n",
        "## TODO: REFERENCE\n",
        "def zeroPadding(l, fillvalue=PAD_CHAR):\n",
        "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
        "\n",
        "def binaryMatrix(l, value=PAD_CHAR):\n",
        "    m = []\n",
        "    for i, seq in enumerate(l):\n",
        "        m.append([])\n",
        "        for token in seq:\n",
        "            if token == PAD_CHAR:\n",
        "                m[i].append(0)\n",
        "            else:\n",
        "                m[i].append(1)\n",
        "    return m\n",
        "\n",
        "def plot(train_loss, val_loss):\n",
        "    plt.plot(train_loss, label='Train')\n",
        "    plt.plot(val_loss, label='Val')\n",
        "    plt.title('Training vs Validation Loss')\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "3MU93iEl0rI8",
        "papermill": {
          "duration": 0.042385,
          "end_time": "2024-11-08T17:57:58.914060",
          "exception": false,
          "start_time": "2024-11-08T17:57:58.871675",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-11-12T12:52:08.773236Z",
          "iopub.execute_input": "2024-11-12T12:52:08.773712Z",
          "iopub.status.idle": "2024-11-12T12:52:08.805461Z",
          "shell.execute_reply.started": "2024-11-12T12:52:08.773680Z",
          "shell.execute_reply": "2024-11-12T12:52:08.804462Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "3MU93iEl0rI8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6"
      ],
      "metadata": {
        "id": "86uCZrsdDVtC"
      },
      "id": "86uCZrsdDVtC"
    },
    {
      "cell_type": "code",
      "source": [
        "with open(GLOVE_EMB, 'r', encoding='utf-8') as f:\n",
        "    glove = f.readlines()"
      ],
      "metadata": {
        "id": "b8IhA-AF5YIh",
        "papermill": {
          "duration": 16.764679,
          "end_time": "2024-11-08T17:58:15.689231",
          "exception": false,
          "start_time": "2024-11-08T17:57:58.924552",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-11-12T12:52:08.806569Z",
          "iopub.execute_input": "2024-11-12T12:52:08.806903Z",
          "iopub.status.idle": "2024-11-12T12:52:22.518488Z",
          "shell.execute_reply.started": "2024-11-12T12:52:08.806858Z",
          "shell.execute_reply": "2024-11-12T12:52:22.517645Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "b8IhA-AF5YIh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7"
      ],
      "metadata": {
        "id": "RK0MFXAMDVtC"
      },
      "id": "RK0MFXAMDVtC"
    },
    {
      "cell_type": "code",
      "source": [
        "#Train and Validation split\n",
        "X_Test = pre_process(np.array(X_Test), max_len=MAX_ARTICLE_LEN)\n",
        "Y_Test = pre_process(np.array(Y_Test), max_len=MAX_DECODER_OUTPUT)\n",
        "Y_Test = list(map(lambda x: f\"{START_CHAR} {x} {END_CHAR}\", Y_Test))\n",
        "\n",
        "X_Test, X_Val, Y_Test, Y_Val = train_test_split(X_Test, Y_Test, test_size=0.3, random_state=23, shuffle=True)\n",
        "print(f'\\nTotal # of stories: {len(X_Test)}')"
      ],
      "metadata": {
        "id": "q-FBHCVHirKK",
        "outputId": "321bdab4-ffc8-40f5-8343-1bb02ae0e279",
        "papermill": {
          "duration": 13.883843,
          "end_time": "2024-11-08T17:58:29.615079",
          "exception": false,
          "start_time": "2024-11-08T17:58:15.731236",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-11-12T12:52:22.528848Z",
          "iopub.execute_input": "2024-11-12T12:52:22.529222Z",
          "iopub.status.idle": "2024-11-12T12:52:36.666799Z",
          "shell.execute_reply.started": "2024-11-12T12:52:22.529172Z",
          "shell.execute_reply": "2024-11-12T12:52:36.665807Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "100%|██████████| 11490/11490 [00:12<00:00, 940.31it/s]\n100%|██████████| 11490/11490 [00:01<00:00, 8467.67it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nTotal # of stories: 8043\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ],
      "id": "q-FBHCVHirKK"
    },
    {
      "cell_type": "code",
      "source": [
        "max(list(map(len, X_Test)))"
      ],
      "metadata": {
        "id": "AThgmFdrpGX0",
        "outputId": "2fd0b077-0fd8-43bd-bb66-d17fe0907c3c",
        "papermill": {
          "duration": 0.031217,
          "end_time": "2024-11-08T17:58:29.667492",
          "exception": false,
          "start_time": "2024-11-08T17:58:29.636275",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-11-12T12:52:36.669868Z",
          "iopub.execute_input": "2024-11-12T12:52:36.670174Z",
          "iopub.status.idle": "2024-11-12T12:52:36.677344Z",
          "shell.execute_reply.started": "2024-11-12T12:52:36.670142Z",
          "shell.execute_reply": "2024-11-12T12:52:36.676402Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 12,
          "output_type": "execute_result",
          "data": {
            "text/plain": "2025"
          },
          "metadata": {}
        }
      ],
      "id": "AThgmFdrpGX0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Features"
      ],
      "metadata": {
        "id": "IW5ssqvr4iju",
        "papermill": {
          "duration": 0.020781,
          "end_time": "2024-11-08T17:58:29.709094",
          "exception": false,
          "start_time": "2024-11-08T17:58:29.688313",
          "status": "completed"
        },
        "tags": []
      },
      "id": "IW5ssqvr4iju"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 8"
      ],
      "metadata": {
        "id": "RqY175u6DVtG"
      },
      "id": "RqY175u6DVtG"
    },
    {
      "cell_type": "code",
      "source": [
        "train_word_vector, train_word_idx_dict, train_idx_word_dict = create_word_vec(X_Test, Y_Test, force_create=FORCE_CREATE_DICT)\n",
        "print(f'Word Vector Shape: {train_word_vector.shape}')\n",
        "assert train_word_vector.shape == (len(train_idx_word_dict.keys()), EMB_SIZE)"
      ],
      "metadata": {
        "id": "ZwbYo--Z4j6p",
        "outputId": "1aac1638-f746-45cd-e08f-5fa9f84234d9",
        "papermill": {
          "duration": 84.442932,
          "end_time": "2024-11-08T17:59:54.172887",
          "exception": false,
          "start_time": "2024-11-08T17:58:29.729955",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-11-12T12:52:36.678690Z",
          "iopub.execute_input": "2024-11-12T12:52:36.678976Z",
          "iopub.status.idle": "2024-11-12T12:54:02.625198Z",
          "shell.execute_reply.started": "2024-11-12T12:52:36.678946Z",
          "shell.execute_reply": "2024-11-12T12:54:02.624270Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Saved embedding dictionary\nWord Vector Shape: (68168, 300)\n",
          "output_type": "stream"
        }
      ],
      "id": "ZwbYo--Z4j6p"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the embedding layer weights based on the pre-trained word vector\n",
        "def create_pretrained_emb_layer(word_vector):\n",
        "    # vocab_size, embed_dim = word_vector.shape\n",
        "    embed_layer = nn.Embedding.from_pretrained(torch.tensor(word_vector).float(), freeze=False)\n",
        "    return embed_layer"
      ],
      "metadata": {
        "id": "WuyIwNIYiZG1",
        "papermill": {
          "duration": 0.028282,
          "end_time": "2024-11-08T17:59:54.221882",
          "exception": false,
          "start_time": "2024-11-08T17:59:54.193600",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-11-12T12:54:02.627804Z",
          "iopub.execute_input": "2024-11-12T12:54:02.628110Z",
          "iopub.status.idle": "2024-11-12T12:54:02.632866Z",
          "shell.execute_reply.started": "2024-11-12T12:54:02.628080Z",
          "shell.execute_reply": "2024-11-12T12:54:02.631830Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "WuyIwNIYiZG1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "STep 9"
      ],
      "metadata": {
        "id": "fcEuT9AfDVtH"
      },
      "id": "fcEuT9AfDVtH"
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, hidden_dim, embedding_layer, n_layers=1, dropout=0):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.embedding = embedding_layer\n",
        "        self.gru = nn.GRU(EMB_SIZE,\n",
        "                          hidden_dim,\n",
        "                          num_layers=n_layers,\n",
        "                          dropout=dropout,\n",
        "                          bidirectional=True)\n",
        "\n",
        "    def forward(self, x, lengths, x_h=None):\n",
        "        embedded = self.embedding(x)\n",
        "        # Pack padded batch of sequences for RNN module\n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, lengths, enforce_sorted=False)\n",
        "        # Forward pass through GRU\n",
        "        y_hat, x_h = self.gru(packed, x_h)\n",
        "        # Unpack padding\n",
        "        y_hat, _ = torch.nn.utils.rnn.pad_packed_sequence(y_hat)\n",
        "        # Sum bidirectional GRU outputs\n",
        "        y_hat = y_hat[:, :, :self.hidden_dim] + y_hat[:, : ,self.hidden_dim:]\n",
        "        # Return output and final hidden state\n",
        "\n",
        "        return y_hat, x_h\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.attention = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.value = nn.Parameter(torch.FloatTensor(hidden_dim).normal_())\n",
        "\n",
        "    def forward(self, x_h, encoder_output):\n",
        "        energy = self.attention(torch.cat((x_h.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
        "        attn_score = torch.sum(self.value * energy, dim=2)\n",
        "\n",
        "        # Transpose max_length and batch_size dimensions\n",
        "        attn_score = attn_score.t()\n",
        "\n",
        "        # the softmax normalized probability scores (with added dimension)\n",
        "        attn_weights = F.softmax(attn_score, dim=1).unsqueeze(1)\n",
        "        return attn_weights\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_dim, output_dim, embedding_layer, n_layers=1, dropout=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.embedding = embedding_layer # TODO\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.n_layers = n_layers\n",
        "        self.gru = nn.GRU(input_size=EMB_SIZE,\n",
        "                          hidden_size=hidden_dim,\n",
        "                          num_layers=n_layers,\n",
        "                          dropout=dropout)\n",
        "\n",
        "        self.attn_gru_combined = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.out = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "        self.attention = Attention(hidden_dim)\n",
        "\n",
        "    def forward(self, x, x_h, encoder_output):\n",
        "        embedded = self.embedding(x)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        y_hat, x_h = self.gru(embedded, x_h)\n",
        "\n",
        "        attn_weights = self.attention(y_hat, encoder_output)\n",
        "\n",
        "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
        "        context = attn_weights.bmm(encoder_output.transpose(0, 1))\n",
        "\n",
        "        # Concatenate weighted context vectors\n",
        "        y_hat = y_hat.squeeze(0)\n",
        "        context = context.squeeze(1)\n",
        "        weighted_context = torch.cat((y_hat, context), 1)\n",
        "\n",
        "        y_hat = torch.tanh(self.attn_gru_combined(weighted_context))\n",
        "\n",
        "        y_hat = F.softmax(self.out(y_hat), dim=1)\n",
        "        return y_hat, x_h"
      ],
      "metadata": {
        "id": "hci0r8KJaxST",
        "papermill": {
          "duration": 0.040589,
          "end_time": "2024-11-08T17:59:54.283229",
          "exception": false,
          "start_time": "2024-11-08T17:59:54.242640",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-11-12T12:54:02.634430Z",
          "iopub.execute_input": "2024-11-12T12:54:02.634800Z",
          "iopub.status.idle": "2024-11-12T12:54:02.652430Z",
          "shell.execute_reply.started": "2024-11-12T12:54:02.634757Z",
          "shell.execute_reply": "2024-11-12T12:54:02.651602Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "hci0r8KJaxST"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 10"
      ],
      "metadata": {
        "id": "Skh9LOyIDVtI"
      },
      "id": "Skh9LOyIDVtI"
    },
    {
      "cell_type": "code",
      "source": [
        "def maskNLLLoss(inp, target, mask):\n",
        "    # TODO: Reference\n",
        "    mask = mask.bool()\n",
        "    nTotal = mask.sum()\n",
        "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
        "    loss = crossEntropy.masked_select(mask).mean()\n",
        "    loss = loss.to(device)\n",
        "    return loss, nTotal.item()\n",
        "\n",
        "def one_pass(x, y, e_model, d_model, e_optim, d_optim, lengths, mask, max_target_len):\n",
        "    e_optim.zero_grad()\n",
        "    d_optim.zero_grad()\n",
        "\n",
        "    x = torch.tensor(x).to(device)\n",
        "    y = torch.tensor(y).to(device)\n",
        "    mask = mask.to(device)\n",
        "\n",
        "    loss = 0 # loss accumulated from each timestep\n",
        "    pass_loss = [] # loss for one pass\n",
        "\n",
        "    e_output, e_hidden = e_model(x, lengths)\n",
        "\n",
        "    # Create initial decoder input (start with START_CHAR for each sequence)\n",
        "    d_input = torch.LongTensor([[train_word_idx_dict[START_CHAR] for _ in range(BATCH_SIZE)]])\n",
        "    d_input = d_input.to(device)\n",
        "\n",
        "    # Set initial decoder hidden state to the encoder's final hidden state\n",
        "    d_hidden = e_hidden[:d_model.n_layers]\n",
        "\n",
        "    for i in range(max_target_len):\n",
        "        d_output, d_hidden = d_model(d_input, d_hidden, e_output)\n",
        "\n",
        "        if random.random() < EPSILON:\n",
        "            d_input = y[i].view(1, -1)\n",
        "\n",
        "        else:\n",
        "            # TODO\n",
        "            _, topi = d_output.topk(1) # topi = torch.argmax(d_output, axis=1)\n",
        "            d_input = torch.LongTensor([[topi[i][0] for i in range(BATCH_SIZE)]])\n",
        "            d_input = d_input.to(device)\n",
        "\n",
        "        # Calculate and accumulate loss\n",
        "        mask_loss, nTotal = maskNLLLoss(d_output, y[i], mask[i])\n",
        "        loss += mask_loss\n",
        "\n",
        "        pass_loss.append(mask_loss.item())\n",
        "\n",
        "\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(e_model.parameters(), 1)\n",
        "    torch.nn.utils.clip_grad_norm_(d_model.parameters(), 1)\n",
        "\n",
        "    e_optim.step()\n",
        "    d_optim.step()\n",
        "\n",
        "    return sum(pass_loss) / len(pass_loss)\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import torch\n",
        "\n",
        "def load_checkpoint_if_available(e_model, d_model, e_optim, d_optim, model_name):\n",
        "    # Check for existing checkpoints\n",
        "    checkpoint_files = glob.glob(f'/kaggle/working/models/{model_name}/checkpoint_*.pth')\n",
        "    if checkpoint_files:\n",
        "        # Load the latest checkpoint based on creation time\n",
        "        latest_checkpoint = max(checkpoint_files, key=os.path.getctime)\n",
        "        print(f\"Loading checkpoint: {latest_checkpoint}\")\n",
        "        checkpoint = torch.load(latest_checkpoint)\n",
        "\n",
        "        # Load model and optimizer states\n",
        "        e_model.load_state_dict(checkpoint['encoder_model'])\n",
        "        d_model.load_state_dict(checkpoint['decoder_model'])\n",
        "        e_optim.load_state_dict(checkpoint['encoder_optimizer'])\n",
        "        d_optim.load_state_dict(checkpoint['decoder_optimizer'])\n",
        "        start_epoch = checkpoint['epoch'] + 1  # Start from the next epoch\n",
        "        print(f\"Resuming from epoch {start_epoch}\")\n",
        "        return start_epoch\n",
        "    else:\n",
        "        print(\"No checkpoint found. Starting training from scratch.\")\n",
        "        return 1  # Start from the first epoch if no checkpoint is found\n",
        "\n",
        "def train(e_model, d_model, e_optim, d_optim, model_name):\n",
        "    print('Training')\n",
        "    e_model.train()\n",
        "    d_model.train()\n",
        "    ep_loss = []\n",
        "\n",
        "    # Define and create the directory for saving models\n",
        "    save_dir = f'/kaggle/working/models/{model_name}'\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    print(\"Model save directory:\", save_dir)\n",
        "\n",
        "    # Load from checkpoint if available\n",
        "    start_epoch = load_checkpoint_if_available(e_model, d_model, e_optim, d_optim, model_name)\n",
        "\n",
        "    for ep in range(start_epoch, EP + 1):  # Start from loaded epoch if applicable\n",
        "        X_samples = []\n",
        "        Y_samples = []\n",
        "        for _ in range(BATCH_SIZE):\n",
        "            rand_idx = random.randint(0, len(X_Test) - 1)\n",
        "            X_samples.append(X_Test[rand_idx])\n",
        "            Y_samples.append(Y_Test[rand_idx])\n",
        "\n",
        "        # Process input data\n",
        "        indexes_batch = [sentence_to_idx(sentence, train_word_idx_dict) for sentence in X_samples]\n",
        "        lengths = torch.tensor([len(indexes) for indexes in indexes_batch], dtype=torch.int64).cpu()\n",
        "        padList = zeroPadding(indexes_batch, fillvalue=train_word_idx_dict[PAD_CHAR])\n",
        "        X_batch = torch.LongTensor(padList).to(device)\n",
        "\n",
        "        # Process labels\n",
        "        indexes_batch = [sentence_to_idx(sentence, train_word_idx_dict) for sentence in Y_samples]\n",
        "        max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
        "        padList = zeroPadding(indexes_batch, fillvalue=train_word_idx_dict[PAD_CHAR])\n",
        "        mask = torch.BoolTensor(binaryMatrix(padList)).to(device)\n",
        "        Y_batch = torch.LongTensor(padList).to(device)\n",
        "\n",
        "        # Run one training pass\n",
        "        loss = one_pass(X_batch, Y_batch, e_model, d_model, e_optim, d_optim, lengths, mask, max_target_len)\n",
        "        ep_loss.append(loss)\n",
        "\n",
        "        # Print loss every PRINT_EVERY_EP epochs\n",
        "        if ep % PRINT_EVERY_EP == 0 and ep > 0:\n",
        "            print(f'EP:{ep} | Loss: {np.array(ep_loss[-PRINT_EVERY_EP:]).mean():.4f}')\n",
        "\n",
        "        # Save checkpoint at every SAVE_MODEL_EVERY_EP epochs\n",
        "        if ep % SAVE_MODEL_EVERY_EP == 0 and ep > 0:\n",
        "            checkpoint_path = f'{save_dir}/checkpoint_{ep}.pth'\n",
        "            print(f\"Saving checkpoint at epoch {ep} to {checkpoint_path}...\")\n",
        "            torch.save({\n",
        "                'epoch': ep,\n",
        "                'encoder_model': e_model.state_dict(),\n",
        "                'decoder_model': d_model.state_dict(),\n",
        "                'encoder_optimizer': e_optim.state_dict(),\n",
        "                'decoder_optimizer': d_optim.state_dict(),\n",
        "                'loss': loss\n",
        "            }, checkpoint_path)\n",
        "\n",
        "    # Save the final model\n",
        "    final_model_path = f'{save_dir}/final_model.pth'\n",
        "    print(f\"Saving final model to {final_model_path}...\")\n",
        "    torch.save({\n",
        "        'epoch': EP,\n",
        "        'encoder_model': e_model.state_dict(),\n",
        "        'decoder_model': d_model.state_dict(),\n",
        "        'encoder_optimizer': e_optim.state_dict(),\n",
        "        'decoder_optimizer': d_optim.state_dict(),\n",
        "        'loss': loss\n",
        "    }, final_model_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "FncqKGnbg7lQ",
        "papermill": {
          "duration": 0.043979,
          "end_time": "2024-11-08T17:59:54.347889",
          "exception": false,
          "start_time": "2024-11-08T17:59:54.303910",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-11-12T12:54:02.653758Z",
          "iopub.execute_input": "2024-11-12T12:54:02.654034Z",
          "iopub.status.idle": "2024-11-12T12:54:02.681765Z",
          "shell.execute_reply.started": "2024-11-12T12:54:02.653985Z",
          "shell.execute_reply": "2024-11-12T12:54:02.680911Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "FncqKGnbg7lQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "step 11"
      ],
      "metadata": {
        "id": "JQtP02eaDVtK"
      },
      "id": "JQtP02eaDVtK"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "\n",
        "# Set the directory for NLTK data\n",
        "nltk_data_dir = '/kaggle/working/nltk_data'\n",
        "os.makedirs(nltk_data_dir, exist_ok=True)\n",
        "\n",
        "# Set the NLTK_DATA environment variable to point to the new directory\n",
        "os.environ['NLTK_DATA'] = nltk_data_dir\n",
        "\n",
        "# Download the required NLTK packages\n",
        "nltk.download('wordnet', download_dir=nltk_data_dir)\n",
        "nltk.download('omw-1.4', download_dir=nltk_data_dir)  # Optional for better word matching\n",
        "nltk.download('punkt', download_dir=nltk_data_dir)\n",
        "\n",
        "# Check if the downloads are successful by verifying the corpus location\n",
        "print(\"NLTK data directory set to:\", nltk_data_dir)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-12T12:54:22.755519Z",
          "iopub.execute_input": "2024-11-12T12:54:22.755902Z",
          "iopub.status.idle": "2024-11-12T12:54:23.545567Z",
          "shell.execute_reply.started": "2024-11-12T12:54:22.755867Z",
          "shell.execute_reply": "2024-11-12T12:54:23.544631Z"
        },
        "trusted": true,
        "id": "Y75TYKk_DVtL",
        "outputId": "f87fbbd7-5f9e-45ba-b456-bf317a519ae3"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "[nltk_data] Downloading package wordnet to\n[nltk_data]     /kaggle/working/nltk_data...\n[nltk_data] Downloading package omw-1.4 to\n[nltk_data]     /kaggle/working/nltk_data...\n[nltk_data] Downloading package punkt to /kaggle/working/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\nNLTK data directory set to: /kaggle/working/nltk_data\n",
          "output_type": "stream"
        }
      ],
      "id": "Y75TYKk_DVtL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 12"
      ],
      "metadata": {
        "id": "tv2SniLnDVtM"
      },
      "id": "tv2SniLnDVtM"
    },
    {
      "cell_type": "code",
      "source": [
        "# # MAIN DRIVER\n",
        "model_name = 'autoencoder_attn'\n",
        "\n",
        "e_embedding_layer = create_pretrained_emb_layer(train_word_vector)\n",
        "d_embedding_layer = create_pretrained_emb_layer(train_word_vector)\n",
        "\n",
        "assert train_word_vector.shape[0] == len(train_word_idx_dict) == len(train_idx_word_dict)\n",
        "\n",
        "encoder = Encoder(E_HIDDEN_DIM, embedding_layer=e_embedding_layer, n_layers=3, dropout=0.3).to(device)\n",
        "decoder = Decoder(D_HIDDEN_DIM, embedding_layer=d_embedding_layer, output_dim=train_word_vector.shape[0], n_layers=3).to(device)\n",
        "\n",
        "e_optim = optim.Adam(encoder.parameters(), lr=1e-3)\n",
        "d_optim = optim.Adam(decoder.parameters(), lr=1e-3)\n",
        "\n",
        "# Load the latest checkpoint if available\n",
        "save_dir = '/kaggle/working/models/autoencoder_attn'\n",
        "checkpoint_files = glob.glob(f'{save_dir}/checkpoint_*.pth')\n",
        "if checkpoint_files:\n",
        "    latest_checkpoint = max(checkpoint_files, key=os.path.getctime)\n",
        "    print(f\"Loading checkpoint: {latest_checkpoint}\")\n",
        "    checkpoint = torch.load(latest_checkpoint)\n",
        "\n",
        "    # Load model and optimizer states\n",
        "    encoder.load_state_dict(checkpoint['encoder_model'])\n",
        "    decoder.load_state_dict(checkpoint['decoder_model'])\n",
        "    e_optim.load_state_dict(checkpoint['encoder_optimizer'])\n",
        "    d_optim.load_state_dict(checkpoint['decoder_optimizer'])\n",
        "    start_epoch = checkpoint['epoch'] + 1  # Start from the next epoch\n",
        "    print(f\"Resuming training from epoch {start_epoch}\")\n",
        "\n",
        "    # Ensure models are on the correct device\n",
        "    encoder.to(device)\n",
        "    decoder.to(device)\n",
        "else:\n",
        "    start_epoch = 1  # Start from the first epoch if no checkpoint is found\n",
        "    print(\"No checkpoint found. Starting training from scratch.\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-12T12:55:02.076799Z",
          "iopub.execute_input": "2024-11-12T12:55:02.077438Z",
          "iopub.status.idle": "2024-11-12T12:55:02.585352Z",
          "shell.execute_reply.started": "2024-11-12T12:55:02.077396Z",
          "shell.execute_reply": "2024-11-12T12:55:02.584370Z"
        },
        "trusted": true,
        "id": "2SAaUBTaDVtM",
        "outputId": "80db8904-77b3-4a18-cc5c-5230b54e2470"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "No checkpoint found. Starting training from scratch.\n",
          "output_type": "stream"
        }
      ],
      "id": "2SAaUBTaDVtM"
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss = train(encoder, decoder, e_optim, d_optim, model_name=model_name)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-12T12:55:21.208867Z",
          "iopub.execute_input": "2024-11-12T12:55:21.209764Z",
          "iopub.status.idle": "2024-11-12T13:59:10.890425Z",
          "shell.execute_reply.started": "2024-11-12T12:55:21.209720Z",
          "shell.execute_reply": "2024-11-12T13:59:10.889375Z"
        },
        "trusted": true,
        "id": "qWtllDKzDVtO",
        "outputId": "fa0a26d4-3681-4f46-ffe3-91e8c0e57f4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Training\nModel save directory: /kaggle/working/models/autoencoder_attn\nNo checkpoint found. Starting training from scratch.\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_30/1861935463.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x = torch.tensor(x).to(device)\n/tmp/ipykernel_30/1861935463.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  y = torch.tensor(y).to(device)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "EP:50 | Loss: 5.6979\nEP:100 | Loss: 4.2565\nEP:150 | Loss: 4.3401\nEP:200 | Loss: 4.1387\nEP:250 | Loss: 4.1766\nEP:300 | Loss: 4.1981\nEP:350 | Loss: 4.1431\nEP:400 | Loss: 4.1599\nEP:450 | Loss: 4.1007\nEP:500 | Loss: 4.0505\nSaving checkpoint at epoch 500 to /kaggle/working/models/autoencoder_attn/checkpoint_500.pth...\nEP:550 | Loss: 4.0449\nEP:600 | Loss: 4.0140\nEP:650 | Loss: 3.9157\nEP:700 | Loss: 3.8947\nEP:750 | Loss: 4.0075\nEP:800 | Loss: 3.9175\nEP:850 | Loss: 3.8598\nEP:900 | Loss: 3.8715\nEP:950 | Loss: 3.8499\nEP:1000 | Loss: 3.8241\nSaving checkpoint at epoch 1000 to /kaggle/working/models/autoencoder_attn/checkpoint_1000.pth...\nEP:1050 | Loss: 3.7509\nEP:1100 | Loss: 3.6756\nEP:1150 | Loss: 3.8230\nEP:1200 | Loss: 3.7330\nEP:1250 | Loss: 3.7677\nEP:1300 | Loss: 3.6767\nEP:1350 | Loss: 3.6415\nEP:1400 | Loss: 3.6980\nEP:1450 | Loss: 3.5875\nEP:1500 | Loss: 3.6447\nSaving checkpoint at epoch 1500 to /kaggle/working/models/autoencoder_attn/checkpoint_1500.pth...\nEP:1550 | Loss: 3.5836\nEP:1600 | Loss: 3.6268\nEP:1650 | Loss: 3.5869\nEP:1700 | Loss: 3.5931\nEP:1750 | Loss: 3.5659\nEP:1800 | Loss: 3.5140\nEP:1850 | Loss: 3.4773\nEP:1900 | Loss: 3.4942\nEP:1950 | Loss: 3.4542\nEP:2000 | Loss: 3.4747\nSaving checkpoint at epoch 2000 to /kaggle/working/models/autoencoder_attn/checkpoint_2000.pth...\nEP:2050 | Loss: 3.4702\nEP:2100 | Loss: 3.4826\nEP:2150 | Loss: 3.3728\nEP:2200 | Loss: 3.4817\nEP:2250 | Loss: 3.3640\nEP:2300 | Loss: 3.3955\nEP:2350 | Loss: 3.3482\nEP:2400 | Loss: 3.3195\nEP:2450 | Loss: 3.4318\nEP:2500 | Loss: 3.3749\nSaving checkpoint at epoch 2500 to /kaggle/working/models/autoencoder_attn/checkpoint_2500.pth...\nEP:2550 | Loss: 3.3051\nEP:2600 | Loss: 3.2467\nEP:2650 | Loss: 3.3614\nEP:2700 | Loss: 3.2276\nEP:2750 | Loss: 3.2162\nEP:2800 | Loss: 3.3123\nEP:2850 | Loss: 3.2171\nEP:2900 | Loss: 3.2145\nEP:2950 | Loss: 3.2126\nEP:3000 | Loss: 3.1780\nSaving checkpoint at epoch 3000 to /kaggle/working/models/autoencoder_attn/checkpoint_3000.pth...\nEP:3050 | Loss: 3.1490\nEP:3100 | Loss: 3.1582\nEP:3150 | Loss: 3.1968\nEP:3200 | Loss: 3.1671\nEP:3250 | Loss: 3.1685\nEP:3300 | Loss: 3.2606\nEP:3350 | Loss: 3.1261\nEP:3400 | Loss: 3.2040\nEP:3450 | Loss: 3.1129\nEP:3500 | Loss: 3.0899\nSaving checkpoint at epoch 3500 to /kaggle/working/models/autoencoder_attn/checkpoint_3500.pth...\nEP:3550 | Loss: 3.0644\nEP:3600 | Loss: 3.0897\nEP:3650 | Loss: 3.1325\nEP:3700 | Loss: 3.0258\nEP:3750 | Loss: 3.0402\nEP:3800 | Loss: 3.0424\nEP:3850 | Loss: 3.0802\nEP:3900 | Loss: 3.0247\nEP:3950 | Loss: 3.0525\nEP:4000 | Loss: 2.9767\nSaving checkpoint at epoch 4000 to /kaggle/working/models/autoencoder_attn/checkpoint_4000.pth...\nEP:4050 | Loss: 2.9531\nEP:4100 | Loss: 2.9655\nEP:4150 | Loss: 2.9583\nEP:4200 | Loss: 2.9180\nEP:4250 | Loss: 2.9787\nEP:4300 | Loss: 2.9565\nEP:4350 | Loss: 2.9623\nEP:4400 | Loss: 2.9169\nSaving final model to /kaggle/working/models/autoencoder_attn/final_model.pth...\n",
          "output_type": "stream"
        }
      ],
      "id": "qWtllDKzDVtO"
    },
    {
      "cell_type": "code",
      "source": [
        "# Uninstall the previous installed nltk library\n",
        "!pip install -U nltk\n",
        "\n",
        "# This upgraded nltkto version 3.5 in which meteor_score is there.\n",
        "!pip install nltk==3.5"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-12T13:59:10.892291Z",
          "iopub.execute_input": "2024-11-12T13:59:10.892627Z",
          "iopub.status.idle": "2024-11-12T13:59:40.798824Z",
          "shell.execute_reply.started": "2024-11-12T13:59:10.892593Z",
          "shell.execute_reply": "2024-11-12T13:59:40.797811Z"
        },
        "trusted": true,
        "id": "DM_fM2s8DVtP",
        "outputId": "48388d44-c581-4bae-842b-55e253d6ae14"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nCollecting nltk\n  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2024.5.15)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.66.4)\nDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: nltk\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nltk-3.9.1\nCollecting nltk==3.5\n  Downloading nltk-3.5.zip (1.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk==3.5) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk==3.5) (1.4.2)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from nltk==3.5) (2024.5.15)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk==3.5) (4.66.4)\nBuilding wheels for collected packages: nltk\n  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434678 sha256=7b5c86583faf57d63afbd556826bf0b71d1ec588e4a50697644f1e4f06340f3e\n  Stored in directory: /root/.cache/pip/wheels/35/ab/82/f9667f6f884d272670a15382599a9c753a1dfdc83f7412e37d\nSuccessfully built nltk\nInstalling collected packages: nltk\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.9.1\n    Uninstalling nltk-3.9.1:\n      Successfully uninstalled nltk-3.9.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.5 which is incompatible.\ntextblob 0.18.0.post0 requires nltk>=3.8, but you have nltk 3.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nltk-3.5\n",
          "output_type": "stream"
        }
      ],
      "id": "DM_fM2s8DVtP"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert-score"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-12T13:59:40.800467Z",
          "iopub.execute_input": "2024-11-12T13:59:40.800864Z",
          "iopub.status.idle": "2024-11-12T13:59:52.776429Z",
          "shell.execute_reply.started": "2024-11-12T13:59:40.800826Z",
          "shell.execute_reply": "2024-11-12T13:59:52.775447Z"
        },
        "trusted": true,
        "id": "ThCaqYo7DVtQ",
        "outputId": "cd6461d1-ff50-4534-8c3e-839f81f8de7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting bert-score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from bert-score) (2.4.0)\nRequirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from bert-score) (2.2.2)\nRequirement already satisfied: transformers>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from bert-score) (4.45.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bert-score) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from bert-score) (2.32.3)\nRequirement already satisfied: tqdm>=4.31.1 in /opt/conda/lib/python3.10/site-packages (from bert-score) (4.66.4)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from bert-score) (3.7.5)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from bert-score) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->bert-score) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2024.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (2024.6.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.25.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.20.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (1.4.5)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (10.3.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->bert-score) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->bert-score) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->bert-score) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->bert-score) (2024.8.30)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->bert-score) (1.3.0)\nDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: bert-score\nSuccessfully installed bert-score-0.3.13\n",
          "output_type": "stream"
        }
      ],
      "id": "ThCaqYo7DVtQ"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Path to check if the WordNet corpus exists\n",
        "wordnet_path = '/kaggle/working/nltk_data/corpora/wordnet'\n",
        "if os.path.exists(wordnet_path):\n",
        "    print(\"WordNet corpus found in:\", wordnet_path)\n",
        "else:\n",
        "    print(\"WordNet corpus not found. Please check the directory structure.\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-12T14:07:00.743141Z",
          "iopub.execute_input": "2024-11-12T14:07:00.743527Z",
          "iopub.status.idle": "2024-11-12T14:07:00.749483Z",
          "shell.execute_reply.started": "2024-11-12T14:07:00.743488Z",
          "shell.execute_reply": "2024-11-12T14:07:00.748453Z"
        },
        "trusted": true,
        "id": "f8tWgu7TDVtQ",
        "outputId": "64782a5e-e3bf-46a5-927b-84f467e3594a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "WordNet corpus found in: /kaggle/working/nltk_data/corpora/wordnet\n",
          "output_type": "stream"
        }
      ],
      "id": "f8tWgu7TDVtQ"
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from bert_score import score as bert_score\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-12T14:09:45.049765Z",
          "iopub.execute_input": "2024-11-12T14:09:45.050141Z",
          "iopub.status.idle": "2024-11-12T14:09:46.631766Z",
          "shell.execute_reply.started": "2024-11-12T14:09:45.050104Z",
          "shell.execute_reply": "2024-11-12T14:09:46.630981Z"
        },
        "trusted": true,
        "id": "JTsqKABEDVtR"
      },
      "execution_count": null,
      "outputs": [],
      "id": "JTsqKABEDVtR"
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Function to evaluate a single article using the trained encoder and decoder\n",
        "def evaluate(article, encoder, decoder, word_idx_dict, idx_word_dict, device):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Convert article to a list of indices\n",
        "        indexes_batch = [sentence_to_idx(article, word_idx_dict)]\n",
        "        lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "        input_batch = torch.LongTensor(indexes_batch).transpose(0, 1).to(device)\n",
        "\n",
        "        # Pass through the encoder\n",
        "        e_output, e_hidden = encoder(input_batch, lengths)\n",
        "        d_hidden = e_hidden[:decoder.n_layers]\n",
        "\n",
        "        # Initialize decoder input with START_CHAR\n",
        "        d_input = torch.ones(1, 1, device=device, dtype=torch.long) * word_idx_dict[START_CHAR]\n",
        "\n",
        "        # Store the generated tokens\n",
        "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
        "\n",
        "        for _ in range(MAX_DECODER_OUTPUT):\n",
        "            d_output, d_hidden = decoder(d_input, d_hidden, e_output)\n",
        "            scores, d_input = torch.max(d_output, dim=1)\n",
        "            all_tokens = torch.cat((all_tokens, d_input), dim=0)\n",
        "            d_input = torch.unsqueeze(d_input, 0)\n",
        "\n",
        "            # Stop if END_CHAR is generated\n",
        "            if idx_word_dict[d_input.item()] == END_CHAR:\n",
        "                break\n",
        "\n",
        "        # Decode the tokens to words\n",
        "        decoded_words = [idx_word_dict[token.item()] for token in all_tokens]\n",
        "        summary = ' '.join(decoded_words).replace(f' {END_CHAR}', '').strip()\n",
        "\n",
        "    return summary\n",
        "\n",
        "# Step 4: Function to compute evaluation metrics\n",
        "def compute_evaluation_metrics(pred_summaries, true_summaries):\n",
        "    # ROUGE calculation\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    rouge1_scores, rouge2_scores, rougeL_scores = [], [], []\n",
        "\n",
        "    for i in range(len(pred_summaries)):\n",
        "        scores = scorer.score(pred_summaries[i], true_summaries[i])\n",
        "        rouge1_scores.append(scores['rouge1'].fmeasure)\n",
        "        rouge2_scores.append(scores['rouge2'].fmeasure)\n",
        "        rougeL_scores.append(scores['rougeL'].fmeasure)\n",
        "\n",
        "    # METEOR calculation\n",
        "    meteor_scores = [meteor_score([true_summaries[i]], pred_summaries[i]) for i in range(len(pred_summaries))]\n",
        "\n",
        "    # BERTScore calculation\n",
        "    P, R, F1 = bert_score(pred_summaries, true_summaries, lang='en', rescale_with_baseline=False)\n",
        "\n",
        "    # Return computed metrics\n",
        "    return {\n",
        "        'rouge1_fmeasure': rouge1_scores,\n",
        "        'rouge2_fmeasure': rouge2_scores,\n",
        "        'rougeL_fmeasure': rougeL_scores,\n",
        "        'meteor': meteor_scores,\n",
        "        'bertscore_f1': F1.tolist()  # Convert tensor to list for consistency\n",
        "    }\n",
        "\n",
        "# Step 5: Model evaluation setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Sample articles and true summaries for evaluation\n",
        "sample_articles = X_Test[:288]  # Take 1/50 part of test articles\n",
        "true_summaries = Y_Test[:288]\n",
        "\n",
        "# Generate predicted summaries\n",
        "predicted_summaries = [evaluate(article, encoder, decoder, train_word_idx_dict, train_idx_word_dict, device) for article in sample_articles]\n",
        "\n",
        "# Compute evaluation metrics\n",
        "metrics = compute_evaluation_metrics(predicted_summaries, true_summaries)\n",
        "\n",
        "# Print evaluation results in percentage format\n",
        "print(\"Evaluation Metrics:\")\n",
        "print(f\"Average ROUGE-1 F1: {np.mean(metrics['rouge1_fmeasure']) * 100:.2f}\")\n",
        "print(f\"Average ROUGE-2 F1: {np.mean(metrics['rouge2_fmeasure']) * 100:.2f}\")\n",
        "print(f\"Average ROUGE-L F1: {np.mean(metrics['rougeL_fmeasure']) * 100:.2f}\")\n",
        "print(f\"Average METEOR: {np.mean(metrics['meteor']) * 100:.2f}\")\n",
        "print(f\"Average BERTScore F1: {np.mean(metrics['bertscore_f1']) * 100:.2f}\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-12T14:43:48.881539Z",
          "iopub.execute_input": "2024-11-12T14:43:48.881947Z",
          "iopub.status.idle": "2024-11-12T14:44:29.903327Z",
          "shell.execute_reply.started": "2024-11-12T14:43:48.881908Z",
          "shell.execute_reply": "2024-11-12T14:44:29.902300Z"
        },
        "trusted": true,
        "id": "iF2Ze3b0DVtR",
        "outputId": "78ec7567-3a11-43e5-d2a8-b59bab5f7d41"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Evaluation Metrics:\nAverage ROUGE-1 F1: 14.32\nAverage ROUGE-2 F1: 2.21\nAverage ROUGE-L F1: 12.01\nAverage METEOR: 9.23\nAverage BERTScore F1: 75.06\n",
          "output_type": "stream"
        }
      ],
      "id": "iF2Ze3b0DVtR"
    }
  ]
}